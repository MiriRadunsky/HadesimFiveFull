# -*- coding: utf-8 -*-
"""partOne.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NB0Vb7QT_Esa-uTyLXZtdDZO3mgH7Hh2

## **part one**
"""

import pandas as pd
import heapq
from collections import Counter
from google.colab import files

uploadedOne = files.upload()

file_path = next(iter(uploadedOne))

total_error_counts = Counter()
chunk_size = 10000

def process_chunk(chunk):
    """ the func processes a chunk from the file and counts thw errors by the type"""
    errors = [line.split(',')[-1].strip() for line in chunk]
    error_counts = Counter(errors)
    total_error_counts.update(error_counts)


with open(file_path, 'r', encoding='utf-8') as file:
    chunk = []
    for i, line in enumerate(file, 1):
        chunk.append(line)

        if i % chunk_size == 0:
            process_chunk(chunk)
            chunk = []

    if chunk:
        process_chunk(chunk)


def print_top_errors(n):
    """the func prints the top n errors"""
    print(f"\nthe {n} common errors in the file")
    top_errors = heapq.nlargest(N, total_error_counts.items(), key=lambda x: x[1])
    for error, count in top_errors:
        print(f"{error}: {count} times")


N = 5
print_top_errors(N)

"""## **סיבוכיות זמן ומקום:**

# סיבוכיות זמן:
 - קריאת הקובץ ופריסת השורות לכל chunk של 10,000 שורות: O(n), כאשר n הוא מספר השורות בקובץ.
 - עיבוד כל chunk והעדכון במילון Counter: O(n).
 - מציאת ה-M השגיאות הנפוצות ביותר עם heapq.nlargest: O(k log M), כאשר k הוא מספר השגיאות הייחודיות ו-M הוא מספר השגיאות הנפוצות ביותר שרוצים להדפיס.

# סיבוכיות מקום:
 - שמירת כל chunk בזיכרון: O(chunk_size).
 - מילון total_error_counts שמכיל את כל השגיאות הייחודיות: O(k).
 - שימוש ב-heapq.nlargest לשמירת רק ה-M השגיאות הנפוצות ביותר: O(M).

# **סיכום:**
 :סיבוכיות הזמן  O(n + k log M) והסיבוכיות מקום: O(k+chunk_size+M)

## **part b**
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import os

file_path = next(iter(uploaded))

if file_path.endswith(".csv"):
    df = pd.read_csv(file_path)
elif file_path.endswith(".parquet"):
    df = pd.read_parquet(file_path)

df.head()

df.info()

def check_invalid_dates(df, date_column="timestamp"):
  """the func checks that the data is in a good format """
  df[date_column] = pd.to_datetime(df[date_column], errors="coerce")
  invalid_dates = df[df[date_column].isna()]
  if not invalid_dates.empty:
    print(invalid_dates)
    df.dropna(subset=[date_column], inplace=True)
    print(df.shape)
  else:
    print("no invalid dates")
  return df

def check_double_data(df):
  """ the func checks that there a no double data"""
  duplicate_rows = df[df.duplicated()]
  if not duplicate_rows.empty:
    print(duplicate_rows)
    df.drop_duplicates(inplace=True)
    print(df.shape)
  else:
    print("no double data")
  return df

def check_nan_or_non_numeric(df, column="value"):
  """the func checks that ther a no nan or not numeric values"""
  nan_values = df[column].isna()
  if nan_values.any():
    print(df[nan_values])
    df.dropna(subset=[column], inplace=True)
    print(df.shape)
  else:
    print("no nan values")
  non_numeric_values = df[~df[column].apply(lambda x: pd.to_numeric(x, errors='coerce')).notna()]
  if not non_numeric_values.empty:
    print(non_numeric_values)
    df.drop(non_numeric_values.index, inplace=True)
    print(df.shape)
  return df

def check_invalidion(df):
  """the func checks that the data is in a good format """
  df = check_invalid_dates(df)
  df = check_double_data(df)
  df = check_nan_or_non_numeric(df)
  return df

df=check_invalidion(df)
df.head()

df.shape

def calculate_hourly_avg(df, time_column="timestamp", value_column="value"):
    """the func calculates the mean valeu per houer """

    df = df.copy()
    df[time_column] = pd.to_datetime(df[time_column])
    df["hour_start"] = df[time_column].dt.floor("H")

    df[value_column] = pd.to_numeric(df[value_column], errors='coerce')

    hourly_avg = df.groupby("hour_start")[value_column].mean().reset_index()
    hourly_avg.columns = ["זמן התחלה", "ממוצע"]

    return hourly_avg

average_per_day=calculate_hourly_avg(df)
average_per_day.head()

def split_and_process_time_series(df, output_folder="daily_files", output_final="results/hourly_averages.csv"):
    """Splits time series data into daily chunks, calculates hourly averages per file, and merges results."""

    df=df.copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["day"] = df["timestamp"].dt.date

    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    if not os.path.exists("results"):
        os.makedirs("results")

    hourly_avgs = []

    for day, day_df in df.groupby("day"):
        daily_file = f"{output_folder}/{day}.csv"
        day_df.to_csv(daily_file, index=False)


        small_df = pd.read_csv(daily_file)
        small_df["timestamp"] = pd.to_datetime(small_df["timestamp"])
        hourly_avg_df = calculate_hourly_avg(small_df, time_column="timestamp", value_column="value")

        hourly_avgs.append(hourly_avg_df)


    final_result = pd.concat(hourly_avgs, ignore_index=True)
    final_result.to_csv(output_final, index=False)




split_and_process_time_series(df)

"""# פתרון לשאלה 3 – איך לחשב ממוצעים שעתיים בזמן אמת כאשר הנתונים מוזרמים בגישת stream:

  לעבד כל רשומה באופן מיידי כאשר היא מתקבלת,
 ולאחסן מידע מצטבר עבור כל שעה.
 עבור כל שעה נשמור את סכום הערכים שהתקבלו עד כה ואת מספר הרשומות.

 נשתמש במבנה נתונים כמו מילון, שבו המפתח הוא שעת ההתחלה (למשל '6:00'),
 והערך הוא סכום הערכים + ספירת רשומות.

 בכל פעם שמתקבלת רשומה:
 1. נחלץ את השעה מתוך ה-timestamp.
 2. נעדכן את הסכום והספירה עבור אותה שעה.
 3. נחשב את הממוצע בריצה לפי: סכום חלקי כמות.

 כך נוכל לקבל בכל רגע את הממוצע העדכני לכל שעה, מבלי להמתין לסיום הזרם כולו.

 דוגמה לרשומה: timestamp = '6:10:00 2025-06-10', value = 12.6
 לאחר עיבוד הרשומה נוסיף את 12.6 לסכום של השעה 6:00 ונגדיל את הספירה באחד.

#   יתרונות Parquet


1.  ביצועי שאילתות בצורה מהירה: ניתן לגשת ישירות לעמודות ספציפיות מבלי לקרוא את כל הקובץ,
   מה שמייעל מאוד ביצועים, במיוחד בקבצים גדולים

2. דחיסת נתונים:
    קובצי Parquet דחוסים באופן אוטומטי, ולכן תופסים פחות מקום בדיסק
    ומאפשרים עיבוד מהיר יותר של נתונים גדולים.

3. תמיכה במבני נתונים מדויקים:
    הפורמט שומר על טיפוסי הנתונים של העמודות (כמו תאריכים או מספרים עשרוניים),
    מה שמונע צורך בהמרות ידניות כפי שלעיתים נדרש ב-CSV.
"""

